{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAKSSkP-kSgQ"
   },
   "source": [
    "# `Промышленное машинное обучение на Spark`\n",
    "## `Задание 02: Spark DataFrame`\n",
    "\n",
    "<span style=\"color:red\">Дедлайн: . 00:00</span>\n",
    "\n",
    "<span style=\"color:red\">Заполненный ноутбук присылать на почту <b>ekolmagorov98@yandex.ru</b> с темой письма <b>[HSE Spark 2024][Задание 02][ФИО]</b>.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Часть 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T04:35:47.363021Z",
     "start_time": "2023-05-03T04:35:42.321492Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35686,
     "status": "ok",
     "timestamp": 1674879052706,
     "user": {
      "displayName": "Максим Находнов",
      "userId": "13074568639193503628"
     },
     "user_tz": -180
    },
    "id": "U8eEHwbzkYE0",
    "outputId": "75035983-645a-4a4b-cd84-7df27abf9f5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyspark in /home/diwert/.local/lib/python3.10/site-packages (3.5.0)\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /home/diwert/.local/lib/python3.10/site-packages (from pyspark) (0.10.9.7)\n",
      "Collecting numpy<2,>=1.16.6\n",
      "  Downloading numpy-1.26.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=4d765d25d8b1579b7a0e95a4907d0c3f7edbd860b4dfe2d9cfb15500a48c69c6\n",
      "  Stored in directory: /home/diwert/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
      "Successfully built wget\n",
      "Installing collected packages: wget, numpy, pyarrow\n",
      "Successfully installed numpy-1.26.3 pyarrow-15.0.0 wget-3.2\n"
     ]
    }
   ],
   "source": [
    "! pip3 install pyspark pyarrow wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvUcUgapkSgU"
   },
   "source": [
    "Скачаем и распакуем данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-03T04:35:49.958998Z",
     "start_time": "2023-05-03T04:35:48.472191Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1 / unknown"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "import zipfile\n",
    "\n",
    "filename = wget.download(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    ")\n",
    "with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall('./smsspamcollection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9140,
     "status": "ok",
     "timestamp": 1674879061842,
     "user": {
      "displayName": "Максим Находнов",
      "userId": "13074568639193503628"
     },
     "user_tz": -180
    },
    "id": "mwAt3F5OkSgW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/11 20:47:51 WARN Utils: Your hostname, vm-03 resolves to a loopback address: 127.0.1.1; using 10.128.0.8 instead (on interface eth0)\n",
      "24/02/11 20:47:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/11 20:47:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.types as T\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = (\n",
    "    SparkConf()\n",
    "        .set('spark.ui.port', '4050')\n",
    "        .setMaster('local[*]')\n",
    ")\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 01:** Считайте DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "path = './smsspamcollection/SMSSpamCollection'\n",
    "spam_df = spark.read.csv(path, sep='\\t', inferSchema=True, header=False)\n",
    "spam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 02:** Назовите колонки таблицы `class` и `text`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[class: string, text: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "spam_df = spam_df.withColumnRenamed(\"_c0\", \"class\").withColumnRenamed(\"_c1\", \"text\")\n",
    "spam_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 03:** Выведите первые $3$ строки датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|class|                text|\n",
      "+-----+--------------------+\n",
      "|  ham|Go until jurong p...|\n",
      "|  ham|Ok lar... Joking ...|\n",
      "| spam|Free entry in 2 a...|\n",
      "+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "spam_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 04:** Создайте две новых колонки: `words` — список слов в тексте и `length` — длина данного списка. Реализуйте запрос за одно присваивание:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------+\n",
      "|class|                text|               words|length|\n",
      "+-----+--------------------+--------------------+------+\n",
      "|  ham|Go until jurong p...|[Go, until, juron...|    20|\n",
      "|  ham|Ok lar... Joking ...|[Ok, lar..., Joki...|     6|\n",
      "| spam|Free entry in 2 a...|[Free, entry, in,...|    28|\n",
      "+-----+--------------------+--------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "spam_df = (\n",
    "            spam_df\n",
    "                .withColumn('words',  F.split(spam_df['text'], ' '))\n",
    "                .withColumn('length', F.size('words'))\n",
    "           )\n",
    "spam_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 05:** Посчитайте среднюю длину текстов в зависимости от класса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+\n",
      "|class|       mean_length|\n",
      "+-----+------------------+\n",
      "|  ham|14.427594779366066|\n",
      "| spam| 23.95314591700134|\n",
      "+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "spam_df.groupBy('class').agg(F.avg('length').alias('mean_length')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 06:** Выведите примерно $0.1\\%$ самых длинных текстов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+------+\n",
      "|class|                text|               words|length|\n",
      "+-----+--------------------+--------------------+------+\n",
      "|  ham|For me the love s...|[For, me, the, lo...|   171|\n",
      "|  ham|The last thing i ...|[The, last, thing...|   162|\n",
      "|  ham|Sad story of a Ma...|[Sad, story, of, ...|   125|\n",
      "|  ham|Sad story of a Ma...|[Sad, story, of, ...|   125|\n",
      "|  ham|How to Make a gir...|[How, to, Make, a...|   121|\n",
      "+-----+--------------------+--------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "spam_df.sort('length', ascending=False).show(int(spam_df.count() * 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 07:** Найдите $10$ слов с самой высокой Document Frequency (количество смс, в которых слово встречается хотя бы раз):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "|  to| 1514|\n",
      "| you| 1168|\n",
      "|   I| 1060|\n",
      "|   a| 1000|\n",
      "| the|  897|\n",
      "| and|  667|\n",
      "|  in|  666|\n",
      "|  is|  597|\n",
      "|   i|  573|\n",
      "| for|  539|\n",
      "+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "counts_df = ( \n",
    "                spam_df\n",
    "                    .select(F.explode(F.col(\"words\")).alias(\"word\"), F.col('text'))\n",
    "                    .groupby(\"word\")\n",
    "                    .agg(F.countDistinct(\"text\").alias(\"count\"))\n",
    "                    .sort(F.col(\"count\").desc())\n",
    "           )\n",
    "counts_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 08:** Положим, что заданы две матрицы $A$, $B$ в координатном виде. Реализуйте перемножение матриц:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  i|  j|  v|\n",
      "+---+---+---+\n",
      "|  0|  0|154|\n",
      "|  0|  1| 97|\n",
      "|  0|  2|163|\n",
      "|  0|  3|159|\n",
      "|  0|  4|  8|\n",
      "|  1|  0| 82|\n",
      "|  1|  1| 63|\n",
      "|  1|  2| 95|\n",
      "|  1|  3| 87|\n",
      "|  1|  4|  2|\n",
      "|  2|  0| 66|\n",
      "|  2|  1| 58|\n",
      "|  2|  2| 88|\n",
      "|  2|  3| 72|\n",
      "|  2|  4|  2|\n",
      "|  3|  0| 87|\n",
      "|  3|  1| 54|\n",
      "|  3|  2| 96|\n",
      "|  3|  3| 90|\n",
      "|  3|  4|  6|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4 x 3\n",
    "A_data = [\n",
    "    (0, 0, 8), (0, 1, 5), (0, 2, 8),\n",
    "    (1, 0, 2), (1, 1, 5), (1, 2, 6),\n",
    "    (2, 0, 2), (2, 1, 6), (2, 2, 4),\n",
    "    (3, 0, 6), (3, 1, 3), (3, 2, 3),\n",
    "]\n",
    "\n",
    "# 3 x 5\n",
    "B_data = [\n",
    "    (0, 0, 9), (0, 1, 4), (0, 2, 9), (0, 3, 9), (0, 4, 1),\n",
    "    (1, 0, 2), (1, 1, 5), (1, 2, 7), (1, 3, 3), (1, 4, 0),\n",
    "    (2, 0, 9), (2, 1, 5), (2, 2, 7), (2, 3, 9), (2, 4, 0),\n",
    "]\n",
    "\n",
    "A = spark.createDataFrame(A_data, schema=['i', 'j', 'v'])\n",
    "B = spark.createDataFrame(B_data, schema=['i', 'j', 'v'])\n",
    "\n",
    "### YOUR CODE HERE\n",
    "# Преобразуем матрицу B для удобства перемножения\n",
    "B_transformed = B.selectExpr('i as i_B', 'j as j_B', 'v as v_B')\n",
    "\n",
    "# Создаем все возможные комбинации строк из A и B_transformed\n",
    "cross_joined = A.join(B_transformed)\n",
    "\n",
    "# Отфильтровываем строки, где j (из A) не равно i_B (из B_transformed)\n",
    "filtered = cross_joined.filter(A.j == B_transformed.i_B)\n",
    "\n",
    "# Вычисляем произведение элементов и суммируем результаты\n",
    "C = (\n",
    "        filtered\n",
    "            .withColumn('v', filtered['v'] * filtered['v_B'])\n",
    "            .groupBy(A.i, B_transformed.j_B)\n",
    "            .agg(F.sum('v').alias('v'))\n",
    "            .withColumnRenamed('j_B', 'j')\n",
    "            .sort(F.col('i'), F.col('j'))\n",
    "    )\n",
    "\n",
    "\n",
    "C.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 09**: Реализуйте перемножение тех же матриц, что и в 8 задании, но используя `spark.sql`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:==========================================================(2 + 0) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  i|  j|  v|\n",
      "+---+---+---+\n",
      "|  0|  0|154|\n",
      "|  0|  1| 97|\n",
      "|  0|  2|163|\n",
      "|  0|  3|159|\n",
      "|  0|  4|  8|\n",
      "|  1|  0| 82|\n",
      "|  1|  1| 63|\n",
      "|  1|  2| 95|\n",
      "|  1|  3| 87|\n",
      "|  1|  4|  2|\n",
      "|  2|  0| 66|\n",
      "|  2|  1| 58|\n",
      "|  2|  2| 88|\n",
      "|  2|  3| 72|\n",
      "|  2|  4|  2|\n",
      "|  3|  0| 87|\n",
      "|  3|  1| 54|\n",
      "|  3|  2| 96|\n",
      "|  3|  3| 90|\n",
      "|  3|  4|  6|\n",
      "+---+---+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "A.createOrReplaceTempView(\"A\")\n",
    "B.createOrReplaceTempView(\"B\")\n",
    "\n",
    "C = spark.sql(\"\"\"\n",
    "    SELECT A.i AS i, B.j AS j, SUM(A.v * B.v) AS v\n",
    "    FROM A\n",
    "    JOIN B ON A.j = B.i\n",
    "    GROUP BY A.i, B.j\n",
    "\"\"\")\n",
    "C.sort(F.col('i'), F.col('j')).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
